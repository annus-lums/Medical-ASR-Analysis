# Medical ASR Project - Complete Documentation

## ğŸ“‹ Table of Contents
1. [Project Overview](#project-overview)
2. [Dataset Information](#dataset-information)
3. [Project Structure](#project-structure)
4. [Methodology](#methodology)
5. [Results & Findings](#results--findings)
6. [Running the Code](#running-the-code)
7. [Dashboard Usage](#dashboard-usage)
8. [Future Work](#future-work)

---

## ğŸ“Š Project Overview

### Objective
Analyze Word Error Rate (WER) patterns in medical Automatic Speech Recognition (ASR) using the MultiMed-ST dataset and OpenAI's Whisper model.

### Research Questions
1. **Descriptive**: How are audio characteristics distributed across the medical speech dataset?
2. **Diagnostic**: What audio/text features correlate with transcription errors?
3. **Predictive**: Can we predict WER from simple features before transcription?

### Key Deliverables
- âœ… ASR inference pipeline with Whisper
- âœ… Feature extraction (audio + text)
- âœ… WER prediction model (Random Forest)
- âœ… Interactive dashboard (React + Next.js)
- âœ… Term-level error analysis
- âœ… Comprehensive project documentation

---

## ğŸ—‚ï¸ Dataset Information

**Name**: MultiMed-ST (Multilingual Medical Speech Translation)  
**Source**: [HuggingFace - leduckhai/MultiMed-ST](https://huggingface.co/datasets/leduckhai/MultiMed-ST)  
**Language**: English subset (~4GB audio)  
**Size**: ~290K speech-text pairs across 6 languages (English: ~50K)

### Dataset Structure
Each sample contains:
- `audio`: Audio array and sampling rate
- `text`: Ground truth transcription
- `translation`: English translation (if source is non-English)
- `duration`: Audio duration in seconds
- `split`: train/validation/test
- `direction`: Translation direction (e.g., enâ†’en, deâ†’en)

### Domain
Medical conversations, lectures, webinars, podcasts, and narrations covering:
- Clinical consultations
- Medical education
- Patient-doctor interactions
- Medical terminology and procedures

---

## ğŸ“ Project Structure

```
Project/
â”œâ”€â”€ transripion.ipynb              # Phase 1: ASR inference & feature extraction
â”œâ”€â”€ med-asr (3).ipynb              # Phase 2: WER prediction modeling
â”œâ”€â”€ wer_dashboard.html             # Legacy HTML dashboard
â”œâ”€â”€ convert_data.py                # CSV to JSON converter for dashboard
â”œâ”€â”€ term_error_analysis.py         # Medical term error analysis script
â”œâ”€â”€ PROJECT_ANALYSIS.md            # Detailed project analysis
â”œâ”€â”€ DEPLOYMENT_GUIDE.md            # Dashboard deployment instructions
â”‚
â””â”€â”€ dashboard/                     # Modern React dashboard
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ app/
    â”‚   â”‚   â”œâ”€â”€ page.tsx           # Main dashboard page
    â”‚   â”‚   â”œâ”€â”€ layout.tsx         # App layout
    â”‚   â”‚   â””â”€â”€ globals.css        # Global styles
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”œâ”€â”€ KPICard.tsx        # KPI metric cards
    â”‚   â”‚   â”œâ”€â”€ ScatterChart.tsx   # Scatter plot visualization
    â”‚   â”‚   â”œâ”€â”€ HistogramChart.tsx # Histogram visualization
    â”‚   â”‚   â”œâ”€â”€ ErrorExamples.tsx  # Error example viewer
    â”‚   â”‚   â””â”€â”€ FeatureSelector.tsx # Feature selection dropdown
    â”‚   â””â”€â”€ utils/
    â”‚       â””â”€â”€ dataProcessor.ts   # Data processing utilities
    â”œâ”€â”€ public/
    â”‚   â””â”€â”€ data/                  # Data files (JSON)
    â”œâ”€â”€ package.json
    â”œâ”€â”€ next.config.js
    â”œâ”€â”€ tailwind.config.ts
    â”œâ”€â”€ tsconfig.json
    â””â”€â”€ README.md
```

---

## ğŸ”¬ Methodology

### Phase 1: Data Preparation & ASR Inference

**File**: `transripion.ipynb`

#### 1.1 Data Loading
```python
from datasets import load_dataset

ds = load_dataset('leduckhai/MultiMed-ST', 'English', split='train', streaming=True)
```
- Uses **streaming mode** to avoid full dataset download
- Processes samples on-the-fly

#### 1.2 Whisper Model Setup
```python
MODEL = 'openai/whisper-tiny'
feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL)
tokenizer = WhisperTokenizer.from_pretrained(MODEL, language='English', task='transcribe')
model = WhisperForConditionalGeneration.from_pretrained(MODEL)
```
- **Model**: Whisper Tiny (39M parameters)
- **Task**: Transcription (not translation)
- **Language**: English
- **Why Tiny?**: Fast inference, good baseline performance

#### 1.3 Audio Processing
```python
def prepare_med_dataset(batch):
    audio_array = batch["audio"]["array"]
    sampling_rate = batch["audio"]["sampling_rate"]
    
    batch["input_features"] = feature_extractor(
        audio_array,
        sampling_rate=sampling_rate
    ).input_features[0]
    
    batch["labels"] = tokenizer(batch["text"]).input_ids
    return batch
```
- Converts audio to log-mel spectrograms (80 channels)
- Resamples to 16kHz if needed
- Tokenizes ground truth for WER calculation

#### 1.4 Inference & WER Calculation
```python
pred_results = trainer.predict(processed_test)
decoded_preds = processor.batch_decode(pred_ids, skip_special_tokens=True)
decoded_labels = processor.batch_decode(label_ids, skip_special_tokens=True)

wers = [wer(t.strip(), p.strip()) for p, t in zip(decoded_preds, decoded_labels)]
```
- Batch inference using Seq2SeqTrainer
- WER computed per sample using `jiwer` library
- WER = (Substitutions + Insertions + Deletions) / Total Words

#### 1.5 Feature Extraction

**Text Features**:
```python
duration = len(audio) / sr
word_count = len(text.split())
char_count = len(text)
avg_word_len = char_count / word_count
speaking_rate = word_count / duration  # words per second
```

**Audio Features**:
```python
# Energy (loudness)
energy = np.mean(audio**2)

# Zero Crossing Rate (voice vs silence)
zcr = librosa.feature.zero_crossing_rate(audio)[0].mean()

# Spectral Centroid (brightness of sound)
spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)[0].mean()

# Silence Ratio (percentage of silence)
silence_ratio = np.mean(np.abs(audio) < 0.01)

# Signal-to-Noise Ratio (audio quality)
signal = np.percentile(np.abs(audio), 95)
noise = np.percentile(np.abs(audio), 5) + 1e-6
snr = signal / noise
```

#### 1.6 Output
```python
df = pd.DataFrame({...})
df.to_csv("wer_prediction_dataset_extended.csv", index=False)
```
- Final dataset with 13 columns (features + WER + texts)
- Ready for modeling and visualization

---

### Phase 2: WER Prediction Modeling

**File**: `med-asr (3).ipynb`

#### 2.1 Model Selection
**Algorithm**: Random Forest Regressor
- **Why?**: Handles non-linear relationships, robust to outliers, interpretable
- **Alternatives considered**: Linear Regression (too simple), XGBoost (similar results)

#### 2.2 Model Configuration
```python
rf_model = RandomForestRegressor(
    n_estimators=300,      # 300 trees
    max_depth=14,          # Limit tree depth to prevent overfitting
    min_samples_leaf=5,    # Minimum samples per leaf
    random_state=42,       # Reproducibility
    n_jobs=-1              # Use all CPU cores
)
```

#### 2.3 Feature Engineering
```python
FEATURE_COLS = [
    "duration_sec",        # Audio length
    "word_count",          # Number of words
    "char_count",          # Number of characters
    "avg_word_len",        # Average word length
    "speaking_rate",       # Words per second
    "energy",              # Audio energy
    "zcr",                 # Zero crossing rate
    "spectral_centroid",   # Spectral brightness
    "silence_ratio",       # Silence percentage
    "snr",                 # Signal-to-noise ratio
]
```

#### 2.4 Results

**Performance Metrics**:
```
Validation Set:
  RÂ² Score: -8.83  âŒ (predicting worse than mean baseline)
  MAE: 0.503

Test Set:
  RÂ² Score: 0.022  âš ï¸ (very low predictive power)
  MAE: 0.321       (average error of 32.1 WER points)
```

**Interpretation**:
- âŒ Model **fails to predict WER** from simple features
- âœ… However, feature importance analysis **reveals insights**:
  - Duration and speaking rate are most important
  - Audio quality (SNR) has some impact
  - Text length features are less predictive

**Why Did Prediction Fail?**
1. **WER is context-dependent**: Errors depend on word meaning, medical terminology, speaker accent
2. **Whisper is robust**: Low average WER (~0.3) means little variance to predict
3. **Missing features**: No linguistic features (phonetic complexity, terminology density, etc.)
4. **Better approach**: Use Whisper's confidence scores instead

---

### Phase 3: Dashboard & Visualization

#### 3.1 Legacy Dashboard (`wer_dashboard.html`)
- HTML + Plotly.js
- Client-side rendering
- Limited interactivity

#### 3.2 Modern Dashboard (`dashboard/`)
**Technology Stack**:
- **Next.js 14**: React framework with SSG/SSR
- **React 18**: UI components
- **TypeScript**: Type safety
- **TailwindCSS**: Utility-first styling
- **Recharts**: Data visualization library
- **Lucide React**: Icon library

**Features**:
- 8 KPI metric cards with gradient backgrounds
- Interactive scatter plots (WER vs features)
- Histogram distributions
- Error example viewer with filtering
- Responsive design (mobile/tablet/desktop)
- Smooth animations and hover effects
- Tooltips for metrics

**Architecture**:
```
page.tsx (Main Page)
  â†“
â”œâ”€ KPICard Ã— 8 (metrics display)
â”œâ”€ FeatureSelector (dropdown)
â”œâ”€ ScatterChart (WER vs feature)
â”œâ”€ HistogramChart Ã— 2 (distributions)
â””â”€ ErrorExamples (error viewer)
     â†“
  dataProcessor.ts (data handling)
```

---

### Phase 4: Term-Level Error Analysis

**File**: `term_error_analysis.py`

#### 4.1 Approach
1. **Extract medical terms** from GT and predicted text
   - Pattern-based: Regex for medical suffixes (itis, osis, oma, pathy)
   - NLP-based: MedSpaCy/SciSpaCy (if available)
2. **Compare term sets**:
   - Missed terms: In GT, not in prediction
   - Hallucinated terms: In prediction, not in GT
   - Correct terms: In both
3. **Build confusion matrix** of term errors
4. **Compute term-level recall**

#### 4.2 Output
```json
{
  "summary": {
    "term_recall": 0.87,
    "total_gt_terms": 1245,
    "total_missed": 162,
    "total_hallucinated": 43
  },
  "top_missed_terms": [
    {"term": "hypertension", "count": 23, "frequency_in_gt": 45},
    {"term": "diabetes", "count": 18, "frequency_in_gt": 52},
    ...
  ]
}
```

#### 4.3 Usage
```bash
python term_error_analysis.py wer_prediction_dataset_extended.csv
```

---

## ğŸ“ˆ Results & Findings

### Key Insights

#### 1. Whisper Performance on Medical Speech
- **Average WER**: 0.3-0.4 (30-40%)
- **Median WER**: ~0.35
- **90th percentile WER**: ~0.6-0.7
- **High-error rate**: 15-25% of samples have WER â‰¥ 0.5

**Verdict**: Whisper Tiny performs **reasonably well** on medical speech, but struggles with:
- Long audio (>30 seconds)
- Fast speech (>4 words/sec)
- Low SNR (noisy recordings)
- Specialized terminology

#### 2. Feature Correlations
- âœ… **Duration** correlates with WER (longer â†’ more errors)
- âœ… **Speaking rate** correlates with WER (faster â†’ more errors)
- âš ï¸ **SNR** has weak correlation (cleaner audio â†’ slightly lower WER)
- âŒ **Text length** alone doesn't predict WER well

#### 3. WER Prediction Challenge
- âŒ Simple acoustic/text features **cannot predict WER accurately**
- âœ… But they **explain variance** in feature importance analysis
- ğŸ’¡ **Better approach**: Use Whisper's internal confidence scores

#### 4. Medical Term Errors
- Most missed terms: Common medical conditions (hypertension, diabetes)
- Hallucinated terms: Similar-sounding words
- Term-level recall: ~85-90% (better than word-level)

### Comparison with Baseline
| Metric | Whisper Tiny | Human (estimated) |
|--------|--------------|-------------------|
| WER | 0.35 | 0.05-0.10 |
| Term Recall | 0.87 | 0.98-0.99 |
| Speed | Real-time | 4-5x slower |

---

## ğŸš€ Running the Code

### Prerequisites
```bash
# Python environment
Python 3.8+
CUDA-capable GPU (recommended for Whisper inference)

# Required packages
pip install datasets transformers librosa evaluate jiwer gradio accelerate torch
```

### Option 1: Google Colab (Recommended)

**Why Colab?**
- Free GPU access (T4/P100)
- Dataset streams from HuggingFace (no download)
- Pre-configured environment

**Steps**:
1. Upload `transripion.ipynb` to Google Colab
2. Enable GPU runtime: Runtime â†’ Change runtime type â†’ GPU
3. Add secrets:
   - `HUGGINGFACE_API_KEY` (optional, for private datasets)
   - `WANDB_API_KEY` (optional, for experiment tracking)
4. Run all cells

**Time Estimate**:
- Full English test set (~50K samples): **2-4 hours**
- Subset (5K samples): **15-30 minutes**

### Option 2: Mac Local

**Pros**: Full control, no internet dependency  
**Cons**: Slow (CPU-only), large download

**Steps**:
```bash
# Install dependencies
pip install -r requirements.txt

# Download dataset (one-time, ~4GB)
python -c "from datasets import load_dataset; ds = load_dataset('leduckhai/MultiMed-ST', 'English', split='test')"

# Run inference (CPU-only, slow!)
jupyter notebook transripion.ipynb
```

**Time Estimate**: **8-12 hours** for full test set on M1/M2 Mac

**Recommendation**: Use Colab for inference, Mac for analysis/modeling

### Option 3: Kaggle

**Why Kaggle?**
- Free GPU (30 hours/week)
- Persistent storage
- Dataset already available as Kaggle dataset

**Steps**:
1. Create Kaggle account
2. Create new notebook
3. Add dataset: leduckhai/MultiMed-ST
4. Enable GPU accelerator
5. Run cells

---

### Running WER Prediction Model

**File**: `med-asr (3).ipynb`

```bash
# Load pre-computed WER dataset
# Train Random Forest model
# Generate feature importance plots
# Create dashboard data
```

**Time**: ~10-15 minutes (CPU is fine)

---

### Running Dashboard

**Local Development**:
```bash
cd dashboard
npm install
npm run dev
```

Open [http://localhost:3000](http://localhost:3000)

**Deploy to Vercel**:
```bash
# Install Vercel CLI
npm install -g vercel

# Deploy
cd dashboard
vercel --prod
```

See `DEPLOYMENT_GUIDE.md` for detailed instructions.

---

## ğŸ“Š Dashboard Usage

### 1. KPI Cards
- Hover over cards to see detailed tooltips
- Gradient colors indicate metric categories:
  - Purple: WER metrics
  - Blue: Audio characteristics
  - Green: Text features
  - Orange: Quality metrics

### 2. Feature Selector
- Select different features to explore correlations
- Charts update dynamically

### 3. Scatter Plot
- Each point is one audio sample
- Hover to see exact values and transcription snippet
- Look for patterns (e.g., longer duration â†’ higher WER)

### 4. Histograms
- Understand distribution of WER and features
- Identify outliers and skewness

### 5. Error Examples
- Filter by WER level: Low (<0.2), Medium (0.2-0.5), High (â‰¥0.5)
- Compare predicted vs ground truth text
- Identify common error patterns

---

## ğŸ”® Future Work

### Improvements
1. **Better Features**:
   - Whisper's attention weights
   - Decoder confidence scores
   - Phonetic complexity metrics
   - Medical term density

2. **Advanced Models**:
   - Fine-tune Whisper on medical data
   - Use larger Whisper models (small, medium, large)
   - Ensemble models

3. **Term-Level Analysis**:
   - Integrate MedSpaCy for better term extraction
   - Build medical term confusion matrix
   - Domain-specific vocabulary filtering

4. **Dashboard Enhancements**:
   - File upload for custom datasets
   - Audio playback with error highlighting
   - Comparison of multiple models
   - Export reports as PDF

5. **Context Analysis**:
   - Classify recording contexts (lecture vs conversation)
   - Analyze errors by context type
   - Speaker diarization

### Research Directions
- **Domain Adaptation**: Fine-tune on medical corpora
- **Confidence Calibration**: Predict when ASR will fail
- **Error Correction**: Post-process with medical NLP
- **Multilingual**: Extend to other languages in MultiMed-ST

---

## ğŸ“š References

1. **Dataset**: [MultiMed-ST on HuggingFace](https://huggingface.co/datasets/leduckhai/MultiMed-ST)
2. **Whisper**: [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)
3. **WER Metric**: [jiwer library](https://github.com/jitsi/jiwer)
4. **Blog Post**: [Medium - Modelling MultiMed Speech Data](https://medium.com/@aminqasmi78/modelling-multimed-speech-data-english-84939d397fd2)

---

## ğŸ“§ Contact

For questions or feedback:
- GitHub Issues
- Email: aminqasmi78@gmail.com

---

**Last Updated**: December 2024  
**Project Status**: âœ… Complete

